{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace, get_huggingface_llm_image_uri\n",
    "\n",
    "%cd /home/jerife/krx/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = \"arn:aws:iam::904233131832:role/krx-sagemaker\"\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed_parameters = {\n",
    "  \"deepspeed\": \"./configs/z2_offload_bf16.json\", # deepspeed config file\n",
    "  \"training_script\": \"./scripts/train.py\" # real training script, not entrypoint\n",
    "}\n",
    "\n",
    "steps = 200\n",
    "training_hyperparameters = {\n",
    "    'hub_model_id': \"WhipParty/krx-omni-3\",\n",
    "    'dataset_name': \"jerife/krx-v4.3\",\n",
    "    'wandb_project': \"krx-gemma2-fft-v1\",                          # number of training epochs\n",
    "    'model_name_or_path': \"google/gemma-2-9b-it\",\n",
    "    'wandb_token': \"[WRITE_YOUR_TOKEN_HERE]\",\n",
    "    'hf_token': \"[WRITE_YOUR_TOKEN_HERE]\",\n",
    "    \"output_dir\": \"/opt/ml/model\",\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'per_device_eval_batch_size': 2,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'num_train_epochs': 1,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'eval_steps': steps,\n",
    "    'save_steps': steps,\n",
    "    'logging_steps': 50,\n",
    "    'evaluation_strategy': \"steps\",\n",
    "    'save_strategy': \"steps\",\n",
    "    'load_best_model_at_end': False,\n",
    "    'save_total_limit': 2,\n",
    "    'bf16': True,\n",
    "    'seed': 42,\n",
    "    'is_debug': False,\n",
    "    'push_to_hub': True,\n",
    "    'hub_private_repo': True,\n",
    "    'hub_strategy': \"every_save\", # \"every_save\" or \"end_of_training\"\n",
    "    \"hub_token\": \"[WRITE_YOUR_TOKEN_HERE]\",    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'train_loss',              'Regex': \"'train_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss',               'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_mcqa_accuracy',      'Regex': \"'eval_mcqa_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "job_name = f'{training_hyperparameters[\"hub_model_id\"].split(\"/\")[1]}'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    # env=hub,\n",
    "    entry_point          = 'deepspeed-launcher.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = '.',      # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p4de.24xlarge',   # ml.p4d.24xlarge or ml.g5.48xlarge\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    # volume_size          = 500,\n",
    "    transformers_version = '4.36.0',\n",
    "    pytorch_version      = '2.1.0',\n",
    "    py_version           = 'py310',\n",
    "    hyperparameters      = {\n",
    "      **training_hyperparameters,\n",
    "      **deepspeed_parameters\n",
    "    },   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.fit(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
